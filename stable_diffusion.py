import os
os.environ['HF_HOME'] = './cache/'

from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler
import torch
import torch.nn as nn
import torch.optim as optim


class AudioConditioning(nn.Module):
    def __init__(self, audio_encoder, embedding_dim):
        super(AudioConditioning, self).__init__()
        self.audio_encoder = audio_encoder
        self.fc = nn.Linear(audio_encoder.output_dim, embedding_dim)

    def forward(self, audio_input):
        audio_features = self.audio_encoder(audio_input)
        return self.fc(audio_features)  # Project t


class AudioConditionalUNet(nn.Module):
    def __init__(self, unet, audio_conditioning):
        super(AudioConditionalUNet, self).__init__()
        self.unet = unet
        self.audio_conditioning = audio_conditioning

    def forward(self, latent_image, timestep, audio_input):
        # Generate audio conditioning
        audio_embed = self.audio_conditioning(audio_input)

        # Pass to UNet
        return self.unet(latent_image, timestep, audio_embed)


if __name__ == "__main__":
    model_id = "stabilityai/stable-diffusion-2-1-base"

    scheduler = EulerDiscreteScheduler.from_pretrained(model_id, subfolder="scheduler")
    pipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)
    pipe = pipe.to("cuda")
    print(pipe.unet)
    print(pipe.vae)
    # prompt = "a photo of an astronaut riding a horse on mars"
    # image = pipe(prompt).images[0]  
        
    # image.save("astronaut_rides_horse.png")



    '''
    All of these are generated by GPT
    '''
    # Define components
    # audio_encoder = YourAudioEncoder()
    # audio_conditioning = AudioConditioning(audio_encoder, embedding_dim=768)
    # unet = PretrainedUNet()
    # conditional_unet = AudioConditionalUNet(unet, audio_conditioning)
    # vae = PretrainedVAE()

    # # Optimizer and loss
    # optimizer = optim.AdamW(conditional_unet.parameters(), lr=1e-4)
    # criterion = nn.MSELoss()

    # # Training loop
    # for epoch in range(num_epochs):
    #     for audio, images in dataloader:
    #         # Preprocess audio and images
    #         audio_input = preprocess_audio(audio)
    #         latents = vae.encode(images)

    #         # Add noise
    #         noise = torch.randn_like(latents)
    #         noisy_latents = latents + noise

    #         # Predict noise with conditional UNet
    #         predicted_noise = conditional_unet(noisy_latents, timestep, audio_input)

    #         # Compute loss
    #         loss = criterion(predicted_noise, noise)

    #         # Backpropagation and optimization
    #         optimizer.zero_grad()
    #         loss.backward()
    #         optimizer.step()

    #     print(f"Epoch {epoch+1}, Loss: {loss.item()}")